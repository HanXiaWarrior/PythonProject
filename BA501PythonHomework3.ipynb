{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Question 1:\n",
    "Now you have a list of 1000 numbers. How do you find out the outliers of these numbers?\n",
    " \n",
    "Answer: \n",
    "First, what I am thinking is to highlight keywords in this question. We have 1000 numbers now, and I assume that all these numbers can be integers, decimals, positive or negative. “Outliers” are results we need to find out, but what outliers do you mean? Value of a number is too large or too small? But what is the range of all these 1000 numbers? So here, I have to clarify that outliers mean values of these numbers are either too larger than or too smaller than most other numbers. Based upon different cases, here I just define a threshold -- ə, when the difference square of value of certain number and mean value is larger than the threshold, I will consider it as one outlier.\n",
    " \n",
    "Second, I’m gonna use statistical method to solve this problem. Typically, I will select variance to find out all outliers based upon my above assumptions. We need to calculate mean value of 1000 numbers: ŷ = (N1+N2+...+N1000)/1000, then calculate respective variance from 1 to 1000: ư(N1) = (N1-ŷ)^2. Finally, after we sort variance and compare variance and threshold we set, it’s easy to find out outliers. By the way, the threshold can be determined by observation of all numbers or by range of variance(just delete which is extremely larger than common variance). Double check what I have said and thought, and follow up if there are extra questions from interviewers.\n",
    " \n",
    "Question 2:\n",
    "There are 10 coins. We know that 1 is biased, 9 are fair. The biased coin has p>0.5 to be head. I picked up one coin and toss it 3 times: head, head, and tail. What’s the probability that the coin I picked is the biased one?\n",
    " \n",
    "Answer: \n",
    "First, after reading the question, I assume that we have 9 fair coins and their probability is p’ = 0.5 to be headed. Also, the biased one coin has p>0.5 to be headed. \n",
    " \n",
    "Now, I know that we pick one coin and toss it 3 times, if we want to know the probability of this coin I pick is the biased one, certainly I should use Bayes’s Probability to calculate the answer. \n",
    " \n",
    "Here I define the event -- A: the coin I pick is the biased one; the event -- B: the 3 times toss results are head, head and tail; and the event -- C: we have 10 coins including 1 biased coin and 9 fair coins. So now we want to get P(A|B,C) = P(A,B,C)/P(B,C). Because we have already known event C is ground truth, the event C is independent and P(C) = 1. Now we can get P(A|B,C) = P(A,B)*P(C)/(P(B)*P(C)) = P(A,B)/P(B) = 1/10*p*p*(1-p)/(1/10*p*p*(1-p) + 9/10*½*½*½). Therefore, the probability we want to get is P(A|B,C)=P(A|B)= p^2*(1-p)/(p^2*(1-p)+9/8).\n",
    " \n",
    "Question 3:\n",
    "Explain type I error, type II error, p-value to your daughter.\n",
    " \n",
    "Answer:\n",
    "Type I error is a terminology when we conduct hypothesis testing, that is we know something is true but we use our hypothesis testing to make a false judgement. For example, today is actually sunny and I propose a hypothesis H0 that today is sunny. But based upon some other weather information and calculation, I reject the hypothesis H0 and consider today as rainy, so I make a false judgement. This is called Type I error.\n",
    " \n",
    "Type II error is also a terminology when we conduct hypothesis testing, that is we know something is false but we use our hypothesis testing to make a false judgement. For similar example, today is actually rainy and I propose a hypothesis H0 that today is sunny. But based upon some other weather information and calculation, I do not reject the hypothesis H0 and consider today as sunny, so I make a false judgement. This is called Type II error.\n",
    " \n",
    "P-value is used for hypothesis testing when I need to determine if we reject original hypothesis H0 or not. Usually when P-value is too small (p<5%), we can say that outcome we get from experiments is very extreme and it does not happen most time. So we can reject H0 and recognize that the opposite of original hypothesis is likely to be correct. \n",
    " \n",
    "Question 4:\n",
    "I flip a coin 100 times, 70 times it is head, and 30 times it is tail. Is the coin fair? \n",
    " \n",
    "Answer:\n",
    "Obviously, the outcome of coins thrown belongs to Binomial distribution, and I will use z score and p-value to determine if the coin is fair or not.\n",
    " \n",
    "We know that p(head) = 70/100 = 0.7 and propose a basic assumption that H0: the coin is fair is true. Additionally, we will allow Type I error -- ǝ = 5%. From known table, z* = z(1-5%/2) = z(97.5%) = 1.97\n",
    " \n",
    " According to CLT, sample mean p follows N(0,1). Z-score: z = (p-û)/ sqrt(ơ^2/n) = (0.7-0.5)/sqrt(0.5*0.5/100) = 0.2/sqrt(0.0025) = 0.2/0.05 = 4. When we use z-score and ǝ to find p-value in the normal distribution graph, we can find out that p-value is smaller than 0.1%.\n",
    " \n",
    "Because p-value is extremely smaller than 5%, we can reject H0. In conclusion, the coin is not fair at all based upon our hypothesis testing. So the answer is NO.\n",
    " \n",
    "Question 5:\n",
    "Mike, Lily and Jim are all trying to test if a coin is fair, and each person tries to do this by tossing the coin 10 times and make conclusion based on the toss results. What are the likelihood that at least one of them will say that the coin is not fair, if the coin is in reality, fair? \n",
    " \n",
    "Answer: \n",
    "According to Law of Large numbers and CLT requirements, those three people only toss the coin 10 times, so the N = 10 < 30. We cannot apply CLT and the sample mean will not observe normal distribution, so here we need to clarify it by using student’s T distribution. By the way, since we know the coin is in reality fair, it’s easy to obtain news that the mean of one toss is û= 0.5.\n",
    " \n",
    "Because those three toss coins and make conclusion based on their own respective results, we cannot know exact variance of three samples, that means variance -- ơ is unknown. For T-test, if we assume  ǝ = 5% and df = 10-1=9, we can know T-score = 3.25. If there is one person who rejects H0 and consider the coin as not fair coin, the real t she or he gets from the 10 times sample should be larger than T-score. \n",
    " \n",
    "Now, we need to know if we get head 5 times-10 times, what are the t score of them? For example, if Mike get 6 times head and 4 times tail, Ā = 0.6, û= 0.5, n =10, ơ ^2 = 0.6*0.4 = 0.24, so the t-score = (0.6-0.5)/sqrt(0.24/10) = 0.645. And so on, when Mike gets 7 times head, t-score = 1.381; when Mike gets 8 times head, t-score = 2.37; when Mike get 9 times head, t-score = 4.298 > 3.25. If such case, we could know that only when Mike gets 9 times or 10 times head or tail, he can reject H0, and claim that the coin is not fair. Therefore the P(coin is not fair under 10 times) = 0.2, since we have three different independent people, P(at least one says the coin is not fair) = 1- (1-0.2)^3 = 1 - 0.8^3 = 0.488 = 48.8%. In conclusion, the the likelihood that at least one of them will say that the coin is not fair, if the coin is in reality fair is 48.8%.\n",
    " \n",
    "Question 6:\n",
    "Google has launched another messaging app. How do you measure the success of this app?\n",
    " \n",
    "Answer:\n",
    "First, we need to clarify that what does success mean? More revenue, larger market share, more countries involved, more active users, more registration, more advertisement orders on app, or higher client evaluation than competitors? Also, what information can I know about this messaging app? User interface, functions, price, download platform, customer experience, innovations, usage, marketing channels, etc? And are you looking for comparisons between the old app and this new app, or between this new app and other competitor’s app on the same market?\n",
    " \n",
    "Second, it’s really interesting question. If we consider total amount of new users as a successful standard in this case, it’s necessary to define a scope which is first 6 months when app is launched. Now we can set up A/B testing for this new messaging app. We collect users data of all total amount of new users in first 6 months of this new app and old messaging app of Google. After data gathering, we could conduct hypothesis testing. The H0 is the amount of total new users of new messaging app is larger than that of old app of Google. Then we need to do calculations to prove that we do not reject the hypothesis. \n",
    " \n",
    "Finally, I will tell you the results of testing and make recommendations of how to increase the amount of users if this app is not so satisfied, like changing marketing channels, giving coupons and promotions, modifying designs, etc.\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
